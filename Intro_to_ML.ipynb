{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (Part 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs, Imports, and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scipy pandas scikit-learn matplotlib seaborn[stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Data\n",
    "- Numerical \n",
    "- Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical\n",
    "a = [1, 2, 3, 4, 5] # discrete\n",
    "b = [0.001, 0.002, 0.003, 0.004, 0.005] # continuous\n",
    "\n",
    "# categorical\n",
    "c = ['a', 'b', 'c', 'd', 'e'] # ordinal\n",
    "d = ['cat', 'dog', 'bird', 'fish', 'lizard'] # nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Variables \n",
    "\n",
    "- **Independent Variables** also called input variables, explanatory variables, predictors, features, or attributes.\n",
    "- **Dependent Variables** also called outcome variable, response variable, output variable, or class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "y = [10, 5, 3, 2, 1]\n",
    "\n",
    "# create figure\n",
    "plt.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    marker='x'\n",
    ")\n",
    "\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('Scatter Plot of X vs Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "**Sample Space**: The set of all possible outcomes of an random trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random number generator\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heads']\n",
      "[5 4]\n"
     ]
    }
   ],
   "source": [
    "# consider a coin flip\n",
    "result_c = rng.choice(['heads', 'tails'], size=1) # what is the sample space?\n",
    "\n",
    "# consider a die roll\n",
    "result_d = rng.choice([1, 2, 3, 4, 5, 6], size=2) # what is the sample space?\n",
    "\n",
    "print(result_c)\n",
    "print(result_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distributions (Distributions)\n",
    "\n",
    "\"A distribution is a function that gives the probabilities of occurrence of possible outcomes for an experiment.\"\n",
    "\n",
    "**Examples**:\n",
    "- Uniform Distribution\n",
    "- Normal Distribution\n",
    "- Binomial Distribution\n",
    "- etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# consider a coin flip again\n",
    "\n",
    "result_c = rng.choice(['heads', 'tails'], size=100) \n",
    "\n",
    "head_count = np.count_nonzero(result_c == 'heads')\n",
    "tail_count = np.count_nonzero(result_c == 'tails')\n",
    "\n",
    "plt.bar(x=['heads', 'tails'], height=[head_count, tail_count], width=0.5)\n",
    "\n",
    "plt.show()\n",
    "# what type of distribution are we sampling from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uniform.pdf(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24197072, 0.26608525, 0.28969155, 0.31225393, 0.3332246 ,\n",
       "       0.35206533, 0.36827014, 0.38138782, 0.39104269, 0.39695255,\n",
       "       0.39894228, 0.39695255, 0.39104269, 0.38138782, 0.36827014,\n",
       "       0.35206533, 0.3332246 , 0.31225393, 0.28969155, 0.26608525,\n",
       "       0.24197072])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm.pdf(np.arange(-1,1.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance \n",
    "\n",
    "- **L1 Distance** (i.e. Manhattan Distance)\n",
    "- **L2 Distance** (i.e. Euclidean Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any points $p, q \\in R^2$:\n",
    "\n",
    "$L1(p, q)=|p_1 - q_1| + |p_2 - q_2|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = 2, 10\n",
    "y1, y2 = 3, -1\n",
    "\n",
    "# L1 Distance\n",
    "\n",
    "# 1 dim\n",
    "dist1 = np.abs(x1-x2)\n",
    "\n",
    "# 2 dim\n",
    "dist2 = np.abs(x1 - x2) + np.abs(y1 - y2)\n",
    "\n",
    "print(f'L1 Distance in one dimension: {dist1}')\n",
    "print(f'L1 Distance in two dimensions: {dist2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any points  $p, q \\in R^2$\n",
    "\n",
    "$L2(p, q)=\\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = 2, 10\n",
    "y1, y2 = 3, -1\n",
    "\n",
    "# L2 Distance\n",
    "\n",
    "# 1 dim\n",
    "dist1 = np.sqrt(np.pow(x1 - x2, 2))\n",
    "\n",
    "# 2 dim\n",
    "dist2 = np.sqrt(np.pow(x1 - x2, 2) + np.pow(y1 - y2, 2))\n",
    "\n",
    "print(f'L2 Distance in one dimension: {dist1}')\n",
    "print(f'L2 Distance in two dimensions: {dist2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling (Normalization)\n",
    "\n",
    "- These terms are often interchangeable but they have distinct purposes in Sci-kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [[5, 0.2],\n",
    "     [3, 0.4],\n",
    "     [2, 0.1],\n",
    "     [4, 0.6]]\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale values inbetween 0 and 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "print(scaler.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = x.mean()\n",
    "s = x.std()\n",
    "\n",
    "print(f'Mean of x: {u}')\n",
    "print(f'Std. Deviation of x: {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moves the data distribution. \n",
    "x_scaled = (x - u)/s\n",
    "\n",
    "print(x_scaled.mean()) # approximately 0\n",
    "print(x_scaled.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_scaled = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_scaled.mean())\n",
    "print(x_scaled.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear/Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Introduce parameters (of models).\n",
    "\n",
    "x = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([[10], [7], [5], [3], [1]])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "preds = model.predict(x)\n",
    "\n",
    "plt.scatter(x, y, marker='o', color='black')\n",
    "plt.plot(x, preds, color='blue', linewidth=3);\n",
    "\n",
    "# The parameters of the model\n",
    "print(f\"Slope and y-intercept: W={model.coef_.item()} b={model.intercept_.item()}\")\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, preds))\n",
    "# Proportion of Y that is predictable with X: 1 is perfect prediction\n",
    "print(\"R squared score: %.2f\" % r2_score(y, preds))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression is not an iterative process (i.e. there is no learning occurring)\n",
    "\n",
    "# Lets calculate the coeffecient and the intercept ourselves\n",
    "n = len(x)\n",
    "b = (sum(y)*sum(np.pow(x, 2)) - sum(x)*sum(x*y)) / (n*sum(np.pow(x,2)) - np.pow(sum(x), 2))\n",
    "w = (n*sum(x*y) - sum(x)*sum(y)) / (n*sum(np.pow(x, 2)) - np.pow(sum(x), 2))\n",
    "\n",
    "print(f'Intercept b = {b}')\n",
    "print(f'Coeffecient W = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Introduce splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_diabetes(as_frame=True).frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series with the targets\n",
    "target = df[\"target\"]\n",
    "# Create a new dataframe without the \"target\" column\n",
    "data = df.drop(columns=[\"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model.fit(data, target)\n",
    "\n",
    "preds = model.predict(data)\n",
    "\n",
    "print(\"Coefficients: \\n\", model.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(target, preds))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(target, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(df, kind=\"reg\", y_vars=\"target\", x_vars=data.columns, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitc Regression\n",
    "\n",
    "- Despite its name Logistic Regression is commonly used for classification in ML.\n",
    "- The output of a Logisitc Regression model is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_breast_cancer(as_frame=True).frame # lets explore this dataset \n",
    "\n",
    "# Dataset Link: https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_iris.html#load-iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[\"target\"]\n",
    "\n",
    "data = df.iloc[:, 2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=RANDOM_SEED)\n",
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=data, y=target, data=df, logistic=True, ci=None); # How can we intrepret this model/graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notably, there is no closed-form solution for Logistic Regression (This is a fundamental difference from Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Challenge\n",
    "\n",
    "1. Create Logistic Regression models with 3 different features.\n",
    "2. Graph the results of each.\n",
    "3. Would any of these models be good predictors of whether or not a person has cancer?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "Application of artificial intelligence ( AI ) algorithms that are able to \n",
    "learn from data and imitate the way human beings make decisions without explicit instructions.\n",
    "\n",
    "**Machine Learning Uses:**\n",
    "* Prediction\n",
    "* Image Recognition\n",
    "* Speech recognition\n",
    "* Medical Diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Machine Learning\n",
    "\n",
    "**Supervised Learning:**\n",
    "* System is trained on labeled Data\n",
    "* EX: classification,regression\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "* System is trained on unlabeled data\n",
    "* EX: Clustering, Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Steps to Creating an ML ( Machine Learning ) Model\n",
    "\n",
    "1. Select and Prepare a Dataset\n",
    "2. Choose an algorithm to run on the dataset\n",
    "3. Train the algorithm\n",
    "4. Using & Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bread cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at our features\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Features, aka the columns of our data, represent data variables that help our model learn patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at our target\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Since this is a binary classification problem, we are determining whether or not cancer is benign or malignant\n",
    "# 0 = malignant ( cancerous ), 1 = benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at our class distribiion\n",
    "\n",
    "print(f\"Class distribution: \\n{pd.Series(y).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple decision tree for visualization\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Decision Tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=cancer.feature_names, class_names=cancer.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it with a bigger max_depth\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Fit on the training Data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRICS\n",
    "\n",
    "* When working with classification tasks, we often use metrics based on the proportion of true and false predicitions\n",
    "\n",
    "Example: Breast Cancer Prediction\n",
    "\n",
    "**True Positive:** Amount of correctly identified malignant tumors\n",
    "\n",
    "**False Positive:** Amount of incorrectly identified malignant tumors\n",
    "\n",
    "**True Negative:** Amount of correctly identified benign tumors\n",
    "\n",
    "**False Negative:** Amount of incorrectly identified benign tumors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Prediction Quality\n",
    "# sklearn accuracy_score() returns the accuracy score of the given predictions\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Accuracy Score:**\n",
    "\n",
    "Proportion of all scores that are correct\n",
    "\n",
    "ACC = ( TN + TP ) / ALL SCORES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also use...\n",
    "# DecisionTreeClassifier.score() returns the accuracy score of the given predictions\n",
    "\n",
    "train_score = dt.score(X_train, y_train)\n",
    "test_score = dt.score(X_test, y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating with Classification Report\n",
    "\n",
    "clf_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision:**\n",
    "- Measures the quality / accuracy of the predicitions, AKA, how often the model predicts correctly.\n",
    "\n",
    "( When we predict cancer, how often are we correct ? )\n",
    "\n",
    "TP / ( TP + FP )\n",
    "\n",
    "### **Recall:**\n",
    "- For all of the positive samples, this is the fraction that we correctly classified.\n",
    "\n",
    "( Of all the cancer cases, how many did we catch ? )\n",
    "\n",
    "TP / ( TP + FN )\n",
    "\n",
    "### When to use which ?\n",
    "\n",
    "Precision Priority\n",
    "* Email spam filtering (better to let spam through than block real emails)\n",
    "\n",
    "Recall Priority\n",
    "* Cancer screening (better to have false alarms than miss cancer)\n",
    "* Fraud detection (better to investigate false leads than miss fraud)\n",
    "\n",
    "<br/>\n",
    "\n",
    "**F1-Score**\n",
    "* balanced measure of performance, computes the average of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating with Confusion Matrix\n",
    "\n",
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = c_matrix, display_labels = ['malignant', 'benign'])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model: RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**:\n",
    "* an algorithm that combines the output of multiple trees to reach a result.\n",
    "\n",
    "For classification tasks, the output is the class selected by the most trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit your Random Forest Classifier on the same data\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate, did the RF model perform better than the DT?\n",
    "\n",
    "accuracy_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting vs Overfitting\n",
    "\n",
    "When evaluating the quality of a ML model, it is important to determine the quality of the fit.\n",
    "\n",
    "**Overfitting**:\n",
    "* The model is too complex and fits the training data too closely, thus is unable to make quality predicitons on new data\n",
    "* High Training Accuracy with low testing accuracy\n",
    "\n",
    "\n",
    "**Underfitting**\n",
    "* The model is too simple, and cannot find the underlying patterns of the data\n",
    "* Poor training and testing accuracies\n",
    "\n",
    "In a good balanced model, the algorithm will capture important patterns without memorizing the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "underfit_rf = RandomForestClassifier(\n",
    "    n_estimators=1,  # Very few trees\n",
    "    max_depth=1,     # Very shallow trees\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "underfit_rf.fit(X_train, y_train)\n",
    "rf_pred_train = underfit_rf.predict(X_train)\n",
    "rf_pred_test = underfit_rf.predict(X_test)\n",
    "\n",
    "train_score =  accuracy_score(y_train, rf_pred_train)\n",
    "test_score =  accuracy_score(y_test, rf_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_score:.3f}\")\n",
    "print(f\"Test Accuracy: {test_score:.3f}\")\n",
    "print(f\"Gap: {train_score - test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "good_rf.fit(X_train, y_train)\n",
    "rf_pred_train = good_rf.predict(X_train)\n",
    "rf_pred_test = good_rf.predict(X_test)\n",
    "\n",
    "train_score =  accuracy_score(y_train, rf_pred_train)\n",
    "test_score =  accuracy_score(y_test, rf_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_score:.3f}\")\n",
    "print(f\"Test Accuracy: {test_score:.3f}\")\n",
    "print(f\"Gap: {train_score - test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINI CHALLENGE :)\n",
    "\n",
    "**Hyperparameter Tuning**:\n",
    "* This is a key step in using ML algorithms, where you find the best parameters to use when training a model\n",
    "\n",
    "Manipulate the parameters of the original RandomForestClassifier to achive a better accuracy score.\n",
    "There are many different parameters you can use to help increase RandomForestClassifier...\n",
    "\n",
    "- max_depth: limits how deep the tree can grow \n",
    "- min_samples_split: minimum samples required to split at a node\n",
    "- min_samples_leaf: each leaf must have this many samples\n",
    "- random_state: controls the randomness of a tree\n",
    "- etc.\n",
    "\n",
    "Link to [Hyperparameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#randomforestclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Feature selection is an important step for improving your Machine learning model.\n",
    "By removing unneccesary features ( columns ), sometimes you can improve the performance and runtime of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we will be using a wine quality dataset\n",
    "\n",
    "wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine = pd.read_csv(wine_url, sep=\";\")\n",
    "\n",
    "# Prepare data\n",
    "X = wine.drop('quality', axis=1)\n",
    "y = wine['quality']\n",
    "\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate our prediction accuracy \n",
    "\n",
    "rf_pred_train = rf_clf.predict(X_train)\n",
    "rf_pred_test = rf_clf.predict(X_test)\n",
    "train_score =  accuracy_score(y_train, rf_pred_train)\n",
    "test_score =  accuracy_score(y_test, rf_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_score:.3f}\")\n",
    "print(f\"Test Accuracy: {test_score:.3f}\")\n",
    "print(f\"Gap: {train_score - test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some classifiers such as Random Forest, have built in tools to help you determine feature importance\n",
    "# lets determine the best features using our good_rf model\n",
    "\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "rf_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Challenge :)\n",
    "\n",
    "Using Feature importance, determine top 3 least important features.\n",
    "\n",
    "Then, see what removing them from the data does for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Least Important Features (Random Forest):\")\n",
    "print(rf_importance.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "- Regularization methods are important in both all Statistical Models, and they are of particular interest for Machine Learning. \n",
    "- They help prevent our models from **overfitting** our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets revisit the Random Forest Method\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "\n",
    "# the max depth hyperparameter prevents each tree in our Forest from growing too large, thus preventing the model from defining more complex decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike linear regression, this model is made through an iterative process.\n",
    "\n",
    "The objective is to **minimize** $\\dfrac{1}{N} ||y - X\\beta||^2 + \\alpha |\\beta|$\n",
    "\n",
    "- Note that $||\\hspace{.1cm}||$ is the L2 norm and $|\\hspace{.1cm}|$ is the L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit([[0, 0], [1, 1]], [0, 1])\n",
    "lr_pred = lr.predict([[0, 1]])\n",
    "\n",
    "print(f'Coefficients: {lr.coef_}')\n",
    "print(f'Prediction: {lr_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.05)\n",
    "lasso.fit([[0, 0], [1, 1]], [0, 1]) # note that the data exists in 3D\n",
    "lasso_pred = lasso.predict([[1,0]])\n",
    "\n",
    "print(f'Coefficients: {lasso.coef_}')\n",
    "print(f'Prediction: {lasso_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.loc[:, 'target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = Lasso(alpha=0.1)\n",
    "ls.fit(x_train, y_train)\n",
    "preds = ls.predict(x_test)\n",
    "\n",
    "r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.coef_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "preds = lr.predict(x_test)\n",
    "\n",
    "r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(penalty='l2', max_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset of titanic passengers\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An important first step in Data preprocessing, is identifying the types of features you have\n",
    "\n",
    "##### **Numerical Features**\n",
    "* Represent measurable quantities\n",
    "* Can have numerical calculations performed on them, such as adding, subtracting, and averaging\n",
    "\n",
    "Two types:\n",
    "\n",
    "**Continuous**: any value within a range\n",
    "Ex ) Fare\n",
    "\n",
    "**Discrete**: distinct, seperate values\n",
    "Ex ) Age, or number of cars a person owns\n",
    "\n",
    "##### **Categorical Features**\n",
    "* Data represented by a limited, fixed number of categories or labels\n",
    "* Describe characteristics or qualities\n",
    "\n",
    "Two types:\n",
    "\n",
    "**Nominal**: Categories without any intrinsic order or ranking. Simple labels\n",
    "Ex ) Sex, or the color of an object\n",
    "\n",
    "**Ordinal**: Categories with meaningful order or ranking\n",
    "Ex ) Education Level ( high school, bachelors, masters, PHD )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning - Handling Missing Values\n",
    "\n",
    "Not every dataset will have every single datapoint\n",
    "Missing values can lead to a lot of issues affecting the accuracy of your program\n",
    "\n",
    "Two ways to deal with missing data values:\n",
    "* Drop all columns with missing values\n",
    "* Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, identify the amount of missing values in your dataset\n",
    "# this can be done using the isnull() method, which flags null values\n",
    "\n",
    "print(titanic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 different columns with missing values:\n",
    "* Age \n",
    "* Cabin\n",
    "* Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic strategy for handling missing values is by dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most simple way to drop missing values, would be dropping them all with dropna()\n",
    "\n",
    "titanic_df.dropna(inplace=False)\n",
    "\n",
    "# Using this strategy removes over 700 entries, which is not ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at our columns with missing values, Embarked only has two.\n",
    "# Thus, it would be acceptible to drop only those entries.\n",
    "# This can be done using the subset parameter\n",
    "\n",
    "titanic_df.dropna(subset=['Embarked'], inplace=True)\n",
    "\n",
    "print(titanic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method for handling missing data, is to drop columns outright\n",
    "# Looking at our data, Cabin has 687 missing entries, which is over half our data\n",
    "# Thus, dropping it is reasonable\n",
    "\n",
    "titanic_df.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "print(titanic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy for handling missing values is **Imputation**.\n",
    "\n",
    "With imputation, for each missing value, fill empty spot with a specific number (usually an average).\n",
    "\n",
    "Steps for Imputation:\n",
    "* Sklearn has a method for imputation called SimpleImputer()\n",
    "* Intialize the imputer\n",
    "* Transform the data by assigning an estimated value to each empty spot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# We are going to be targeting the Age column with imputation\n",
    "# Age is a numerical value\n",
    "# We will be using the median age to fill in the missing values\n",
    "\n",
    "numeric_features = ['Age', 'Fare']\n",
    "# We are imputing both Age and Fare, since the simple imputer is expecting a dataFrame input, not a series\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "titanic_df[numeric_features] = imputer.fit_transform(titanic_df[numeric_features])\n",
    "\n",
    "print(titanic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have handled our missing values, we can move on to other preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Handling Categorical Variables\n",
    "\n",
    "Not every variable is going to be numerical.\n",
    "\n",
    "To utilize categorical data, it is important to convert it into forms that the algorithm can use\n",
    "\n",
    "How do we handle this:\n",
    "* Dropping Categorical Variables\n",
    "* Label Encoding\n",
    "* One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and Ticket are irrelevant categorical variables, so they are safe to drop\n",
    "\n",
    "columns_to_drop = ['Name', 'Ticket']\n",
    "\n",
    "titanic_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(titanic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Encoders - Label Encoding\n",
    "\n",
    "“Labels” each value with a distinct value\n",
    "\n",
    "The LabelEncoder() looks at all unique values in the column\n",
    "\n",
    "and alphabetically assigns integers starting from 0\n",
    "\n",
    "Example: ‘Strongly Agree’ = 5 ‘Strongly Disagree’ = 1\n",
    "\n",
    "This approach does not work well if values do not have clear numerical ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# We will be using categorical label feature Sex to demonstrate\n",
    "categorical_features = ['Sex']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    titanic_df[feature + '_encoded'] = label_encoder.fit_transform(titanic_df[feature])\n",
    "    \n",
    "titanic_df[['Sex', 'Sex_encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Encoders - One Hot Encoding\n",
    "\n",
    "Basic Idea: Creating new columns to show the presence/absence of the categorical variables from the original data\n",
    "\n",
    "Does not work well when datasets have large amounts of different categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# We will be using categorical label feature Embarked to demonstrate\n",
    "categorical_features = ['Embarked']\n",
    "\n",
    "oh_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "for feature in categorical_features:\n",
    "\n",
    "    # double brackets since OneHotEncoder expects a dataFrame, not a series\n",
    "    encoded_values = oh_encoder.fit_transform(titanic_df[[feature]])\n",
    "    \n",
    "    # Get the one-hot encoded feature names (e.g., 'Embarked_C', 'Embarked_Q', 'Embarked_S')\n",
    "    encoded_columns = oh_encoder.get_feature_names_out([feature])\n",
    "\n",
    "    # Create new columns in the DataFrame with the one-hot encoded values\n",
    "    encoded_df = pd.DataFrame(encoded_values, columns=encoded_columns)\n",
    "\n",
    "    # Concatenate the encoded columns to the original DataFrame\n",
    "    titanic_df = pd.concat([titanic_df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "print(titanic_df[['Embarked'] + list(encoded_df.columns)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Un-needed columns\n",
    "\n",
    "titanic_df.drop(columns=['Sex', 'Embarked'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scaling / Normalization is a highly reccommended preprocessing step prior to modeling.\n",
    "\n",
    "Many machine learning algorithms like Gradient descent methods, KNN algorithm, linear and logistic regression, etc. require data scaling to produce good results.\n",
    "\n",
    "Your method of Scaling often depends on your data or your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california = fetch_california_housing()\n",
    "california_df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "    \n",
    "california_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features explanation\n",
    "features_to_analyze = {\n",
    "        'MedInc': 'Median Income - Right-skewed',\n",
    "        'AveRooms': 'Average Rooms - Contains outliers',\n",
    "        'Population': 'Population - Large scale differences',\n",
    "        'Latitude': 'Latitude - Bounded geographic',\n",
    "        'HouseAge': 'House Age - Bounded numeric'\n",
    "}\n",
    "\n",
    "analysis_df = california_df[features_to_analyze.keys()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Distributions\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (feature, description) in enumerate(features_to_analyze.items(), 1):\n",
    "    plt.subplot(1, 5, i)\n",
    "    sns.histplot(analysis_df[feature], kde=True)\n",
    "    plt.title(f'{feature}\\n{description.split(\"-\")[0]}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard Scaler\n",
    "\n",
    "* Standard Scaler is a normalization technique that transforms data to have mean=0 and standard dev=1\n",
    "* This process ensures that all features are on the same scale, preventing any single feature from dominating the learning process due to its larger magnitude.\n",
    "\n",
    "\n",
    "The transformation performed by StandardScaler can be expressed mathematically as:\n",
    "\n",
    "z=x−μ/σ​\n",
    "\n",
    "where x represents the original feature value, μ is the mean of the feature, σ is the standard deviation, and z is the standardized feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(analysis_df)\n",
    "stdScaled_df = pd.DataFrame(scaled_data,\n",
    "                         columns=analysis_df.columns)\n",
    "\n",
    "print(stdScaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize new Distributions\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (feature, description) in enumerate(features_to_analyze.items(), 1):\n",
    "    plt.subplot(1, 5, i)\n",
    "    sns.histplot(stdScaled_df[feature], kde=True)\n",
    "    plt.title(f'{feature}\\n{description.split(\"-\")[0]}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably thinking: Okay,,, this looks exactly the same!\n",
    "\n",
    "But take a closer look and see that the range on the X axes have changed.\n",
    "\n",
    "StandardScaler does not alter the shape of the distribution of each feature; it only shifts and scales it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler\n",
    "\n",
    "* MinMaxScaler is a normalization technique that makes a distributions minimum=0 and maximum=1\n",
    "* MinMax Scaler shrinks the data within the given range, and transforms data by scaling features to a given range.\n",
    "\n",
    "Like StandardScaler, this does not change the shape of the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_data = minmax_scaler.fit_transform(analysis_df)\n",
    "minmax_df = pd.DataFrame(minmax_data,\n",
    "                         columns=analysis_df.columns)\n",
    "\n",
    "print(minmax_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize new Distributions\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (feature, description) in enumerate(features_to_analyze.items(), 1):\n",
    "    plt.subplot(1, 5, i)\n",
    "    sns.histplot(minmax_df[feature], kde=True)\n",
    "    plt.title(f'{feature}\\n{description.split(\"-\")[0]}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the graphs, while the shape appears the same, the range on the X axes has changed.\n",
    "* They are all now bounded between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare the Distributions\n",
    "\n",
    "plt.subplot(3, 2, i+1)\n",
    "        \n",
    "        # Plot original and scaled distributions\n",
    "sns.kdeplot(data=analysis_df[feature], label='Original')\n",
    "sns.kdeplot(data=stdScaled_df[feature], label='StandardScaler')\n",
    "sns.kdeplot(data=minmax_df[feature], label='MinMaxScaler')\n",
    "        \n",
    "plt.title(f'{feature} Distribution Comparison')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how this changed the distributions of our features numericallu\n",
    "\n",
    "for feature in features_to_analyze.keys():\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(\"Original Data:\")\n",
    "        print(f\"Range: [{analysis_df[feature].min():.2f}, {analysis_df[feature].max():.2f}]\")\n",
    "        print(f\"Mean: {analysis_df[feature].mean():.2f}\")\n",
    "        print(f\"Std: {analysis_df[feature].std():.2f}\")\n",
    "        \n",
    "        print(\"\\nAfter StandardScaler:\")\n",
    "        std_scaled = stdScaled_df[feature]\n",
    "        print(f\"Range: [{std_scaled.min():.2f}, {std_scaled.max():.2f}]\")\n",
    "        print(f\"Mean: {std_scaled.mean():.2f}\")  # Should be close to 0\n",
    "        print(f\"Std: {std_scaled.std():.2f}\")    # Should be close to 1\n",
    "        \n",
    "        print(\"\\nAfter MinMaxScaler:\")\n",
    "        minmax_scaled = minmax_df[feature]\n",
    "        print(f\"Range: [{minmax_scaled.min():.2f}, {minmax_scaled.max():.2f}]\")\n",
    "        print(f\"Mean: {minmax_scaled.mean():.2f}\")\n",
    "        print(f\"Std: {minmax_scaled.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clustering - with KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Cluster Analysis?\n",
    "\n",
    "Finding simularities between data according to the characteristics found in the data \n",
    "and grouping similar data objects into clusters\n",
    "\n",
    "Cluster analysis is a form of **unsupervised learning**\n",
    "\n",
    "**Cluster:** a collection of data objects\n",
    "* similar to one another in the same cluster\n",
    "* dissimilar to objects in other clusters\n",
    "\n",
    "Applications\n",
    "* Standalone tool\n",
    "* preprocessing step for other algorithms\n",
    "\n",
    "A good clustering method will produce high wuality clutsers with\n",
    "* high intra-class simularity ( less distance between objects in same cluster )\n",
    "* low inter-class simularity ( more distance between objects of different clusters )\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means clustering method\n",
    "\n",
    "Partitioning method: \n",
    "Find a global optimal clustering, aka\n",
    "Given a k, find a partition of k clusters that optimizes the chosen paritioning criterion\n",
    "\n",
    "Given k, the K-means algorithm is implemented in 4 steps:\n",
    "1. Partition objects into k nonempty subsets\n",
    "2. Compute seed points as the centroids of the clusters of the current partition\n",
    "3. Assign each object t the cluster with the nearest seed point\n",
    "4. repeat from step 2 untill no more new assignments\n",
    "\n",
    "Centroid: typically the mean of the points in the cluster\n",
    "\n",
    "Closeless is measured by Euclidean distance, or other equations\n",
    "d = √[ (x2 – x1)^2 + (y2 – y1)^2 ]\n",
    "( pythagoreas theorem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we will be using the iris dataser\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "#Take a look at our data\n",
    "pd.DataFrame(data=X, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at our classes\n",
    "\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For applying distance based algorithms like k-means,\n",
    "# scaling / normalization is reccomended\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the mapping of the original points and their respective classes\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=iris.target, cmap='viridis')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('Original Iris Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform clustering with k-means\n",
    "# We will be using 3 clusters since there are 3 classes\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets plot the points with the kmeans calculated clusters\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "# Grab the kmeans calculated centroids \n",
    "centers = kmeans.cluster_centers_\n",
    "# Converts back to og scale for meaningful visualization in original feature space\n",
    "centers_original = scaler.inverse_transform(centers)\n",
    "plt.scatter(centers_original[:, 0], centers_original[:, 1], \n",
    "            c='red', marker='x', s=200, linewidth=3, label='Centroids')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('K-means Clusters')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINI CHALLENGE :)\n",
    "\n",
    "The kmeans algorithm can result in slightly different calculations each time\n",
    "depending on where the initial randomized centers are placed.\n",
    "\n",
    "Using random_state, this can be manipulated. \n",
    "\n",
    "For this challenge, I want you to adjust the number on the random_state in the kmeans call,\n",
    "and try to find a clustering that matches closer to the original iris classes scatterplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, svm\n",
    "from sklearn.datasets import load_digits\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "clf = svm.SVC(gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Classification report for classifier {clf}:\\n\"\n",
    "    f\"{metrics.classification_report(y_test, predicted)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = clf.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(y_test, predicted, output_dict=True)\n",
    "\n",
    "result = {}\n",
    "result['report'] = report\n",
    "result['params'] = params\n",
    "\n",
    "with open('svm_results.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "- It helps remove reliance/dependence on the particular train/test split made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear', C=1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform cross validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f is the average accuracy, with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lets try fitting the model again, but with another split type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can alos create custom cv strategies\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(n_splits=5, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ss = cross_val_score(model, X_train, y_train, cv=ss)\n",
    "print(scores_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f is the average accuracy, with a standard deviation of %0.2f\" % (scores_ss.mean(), scores_ss.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Challenge\n",
    "\n",
    "1. Using the same dataset, perform cross validation with the StratifiedKFold class.\n",
    "2. Be able to explain the difference between KFOLD and StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (Continued)\n",
    "\n",
    "- CV can be useful for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "ds = fetch_openml(data_id=37)\n",
    "\n",
    "# Dataset Link: https://www.openml.org/search?type=data&sort=runs&status=active&qualities.NumberOfClasses=gte_2&id=1504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.data\n",
    "target = ds.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.apply(lambda label: 1 if label == 'tested_positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(random_state=RANDOM_SEED, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.score(x_train, y_train)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.score(x_test, y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How can cross validation help us? Lets discuss a models Parameter Space?**\n",
    "\n",
    "- **Lets explore a [SVC's](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) Parameter Space.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x = ss.fit_transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, data.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [1, 10, 100, 1000], 'gamma': ['scale', 'auto', 0.001, 0.0001], 'class_weight': ['balanced', None], 'kernel': ['rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(random_state=RANDOM_SEED)\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "grid_searcher = GridSearchCV(model, param_grid=param_grid, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = grid_searcher.cv_results_\n",
    "\n",
    "with open('gs_results.json', 'w') as f:\n",
    "    json.dump(results, f, default=lambda obj: obj.tolist() if isinstance(obj, np.ndarray) else obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by mean_test_score in descending order\n",
    "# Get the index of all the 1s in results['params']\n",
    "top_ranks = sorted([(rank, i) for i, rank in enumerate(results['rank_test_score'])], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "idx = top_ranks[0][1]\n",
    "\n",
    "top_params = results['params'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=10, random_state=RANDOM_SEED)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "tr_score = model.score(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(tr_score)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(random_state=RANDOM_SEED, **top_params)\n",
    "model.fit(x_train, y_train)\n",
    "tr_score = model.score(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(tr_score)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ending Challenge :D\n",
    "\n",
    "Using the loan dataset, use the steps you've learned to evaluate a model.\n",
    "\n",
    "Link to Dataset: https://www.kaggle.com/datasets/ninzaami/loan-predication?resource=download\n",
    "\n",
    "you will..\n",
    "- handle missing data\n",
    "- identify categorical variables\n",
    "- handle categorical values\n",
    "- choose a model we have learned today\n",
    "- evaluate your model\n",
    "\n",
    "Your target to predict: Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df = pd.read_csv('loan.csv')\n",
    "\n",
    "load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
